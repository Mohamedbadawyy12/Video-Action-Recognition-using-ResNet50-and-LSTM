{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# mount drive and inspect dataset structure"
      ],
      "metadata": {
        "id": "VxbLTjrWAAzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uaLqN1b_82i",
        "outputId": "7fc42153-fe5d-4834-b42c-766a1b6494c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Found videos: 11158\n",
            "Example paths (first 10):\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g01_c01.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g01_c02.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g01_c03.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g01_c04.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g01_c05.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g01_c06.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g02_c01.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g02_c02.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g02_c03.avi\n",
            "/content/drive/MyDrive/Datasets/UCF-101/Fencing/v_Fencing_g02_c04.avi\n",
            "Detected class folders count: 88\n",
            "First 20 class folder names: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, sys, glob\n",
        "DATA_ROOT = '/content/drive/MyDrive/Datasets/UCF-101'\n",
        "video_exts = ('.mp4', '.avi', '.mov', '.mkv')\n",
        "\n",
        "def find_videos(root):\n",
        "    vids=[]\n",
        "    for dp, dn, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(video_exts):\n",
        "                vids.append(os.path.join(dp,f))\n",
        "    return vids\n",
        "\n",
        "videos = find_videos(DATA_ROOT)\n",
        "print(\"Found videos:\", len(videos))\n",
        "print(\"Example paths (first 10):\")\n",
        "for v in videos[:10]:\n",
        "    print(v)\n",
        "\n",
        "# try detect class folders (common layout: DATA_ROOT/<class_name>/*.avi)\n",
        "classes = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT,d))])\n",
        "print(\"Detected class folders count:\", len(classes))\n",
        "print(\"First 20 class folder names:\", classes[:20])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " create a small subset metadata"
      ],
      "metadata": {
        "id": "ZQthO0lPIW77"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "N_CLASSES = 25\n",
        "K_PER_CLASS = 200\n",
        "\n",
        "# read videos list from step 1\n",
        "video_exts = ('.mp4', '.avi', '.mov', '.mkv')\n",
        "def find_videos(root):\n",
        "    vids=[]\n",
        "    for dp, dn, files in os.walk(root):\n",
        "        for f in files:\n",
        "            if f.lower().endswith(video_exts):\n",
        "                vids.append(os.path.join(dp,f))\n",
        "    return vids\n",
        "\n",
        "DATA_ROOT = '/content/drive/MyDrive/Datasets/UCF-101'\n",
        "videos = find_videos(DATA_ROOT)\n",
        "\n",
        "# detect classes\n",
        "classes = sorted([d for d in os.listdir(DATA_ROOT) if os.path.isdir(os.path.join(DATA_ROOT,d))])\n",
        "\n",
        "if len(classes) >= N_CLASSES:\n",
        "    sel_classes = classes[:N_CLASSES]\n",
        "else:\n",
        "    parent_names = {}\n",
        "    for v in videos:\n",
        "        parent = os.path.basename(os.path.dirname(v))\n",
        "        parent_names.setdefault(parent, []).append(v)\n",
        "    sel_classes = list(parent_names.keys())[:N_CLASSES]\n",
        "\n",
        "selected_videos = []\n",
        "for c in sel_classes:\n",
        "    cand = []\n",
        "    folder = os.path.join(DATA_ROOT, c)\n",
        "    if os.path.isdir(folder):\n",
        "        for f in os.listdir(folder):\n",
        "            if f.lower().endswith(video_exts):\n",
        "                cand.append(os.path.join(folder,f))\n",
        "    else:\n",
        "        for v in videos:\n",
        "            if os.path.basename(os.path.dirname(v)) == c:\n",
        "                cand.append(v)\n",
        "    if len(cand) > K_PER_CLASS:\n",
        "        cand = random.sample(cand, K_PER_CLASS)\n",
        "    selected_videos += cand\n",
        "\n",
        "print(\"Selected videos:\", len(selected_videos))\n",
        "print(\"Selected classes:\", sel_classes)\n",
        "\n",
        "OUT_META_DIR = '/content/drive/MyDrive/ucf_small'\n",
        "os.makedirs(OUT_META_DIR, exist_ok=True)\n",
        "df = pd.DataFrame([{'video_path': v, 'label': os.path.basename(os.path.dirname(v))} for v in selected_videos])\n",
        "meta_csv = os.path.join(OUT_META_DIR, 'metadata.csv')\n",
        "df.to_csv(meta_csv, index=False)\n",
        "print(\"Wrote metadata to:\", meta_csv)\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GA2RYlLQAO56",
        "outputId": "b03c9104-921c-43b6-a752-37d1c75bed9e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected videos: 3360\n",
            "Selected classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen']\n",
            "Wrote metadata to: /content/drive/MyDrive/ucf_small/metadata.csv\n",
            "                                          video_path           label\n",
            "0  /content/drive/MyDrive/Datasets/UCF-101/ApplyE...  ApplyEyeMakeup\n",
            "1  /content/drive/MyDrive/Datasets/UCF-101/ApplyE...  ApplyEyeMakeup\n",
            "2  /content/drive/MyDrive/Datasets/UCF-101/ApplyE...  ApplyEyeMakeup\n",
            "3  /content/drive/MyDrive/Datasets/UCF-101/ApplyE...  ApplyEyeMakeup\n",
            "4  /content/drive/MyDrive/Datasets/UCF-101/ApplyE...  ApplyEyeMakeup\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "frame sampling helper"
      ],
      "metadata": {
        "id": "BBf7TGH4BdmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def sample_frames(video_path, num_frames=16):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened():\n",
        "        return None\n",
        "    total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    if total <= 0:\n",
        "        cap.release()\n",
        "        return None\n",
        "    indices = np.linspace(0, total-1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            if len(frames) > 0:\n",
        "                frames.append(frames[-1].copy())\n",
        "            else:\n",
        "                frames.append(np.zeros((224,224,3), dtype=np.uint8))\n",
        "            continue\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        frames.append(frame)\n",
        "    cap.release()\n",
        "    return frames\n"
      ],
      "metadata": {
        "id": "h9oPomZ2BaDF"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " extract frame features with ResNet50 and save .npy files"
      ],
      "metadata": {
        "id": "kpivzeFXBiNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from torchvision import models, transforms\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# preprocessing pipeline\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# load ResNet50 and remove final FC\n",
        "resnet = models.resnet50(pretrained=True)\n",
        "modules = list(resnet.children())[:-1]\n",
        "resnet_feat = torch.nn.Sequential(*modules).to(device)\n",
        "resnet_feat.eval()\n",
        "\n",
        "# parameters\n",
        "NUM_FRAMES = 16\n",
        "FEATURE_DIR = '/content/drive/MyDrive/ucf_small/features'\n",
        "os.makedirs(FEATURE_DIR, exist_ok=True)\n",
        "\n",
        "meta_csv = os.path.join('/content/drive/MyDrive/ucf_small', 'metadata.csv')\n",
        "meta = pd.read_csv(meta_csv)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, row in tqdm(meta.iterrows(), total=len(meta)):\n",
        "        vpath = row['video_path']\n",
        "        fname = os.path.splitext(os.path.basename(vpath))[0]\n",
        "        out_file = os.path.join(FEATURE_DIR, fname + '.npy')\n",
        "        if os.path.exists(out_file):\n",
        "            continue\n",
        "        frames = sample_frames(vpath, num_frames=NUM_FRAMES)\n",
        "        if frames is None:\n",
        "            print(\"Skipping (cannot open):\", vpath)\n",
        "            continue\n",
        "        batch = torch.stack([preprocess(f) for f in frames]).to(device)\n",
        "        feats = resnet_feat(batch)  # shape: (T, 2048, 1, 1)\n",
        "        feats = feats.reshape(feats.size(0), -1).cpu().numpy()  # (T, 2048)\n",
        "        np.save(out_file, feats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkYWGbw8BfPe",
        "outputId": "a3b690f6-a903-464b-a088-e7d99fb5bef3"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "100%|██████████| 3360/3360 [46:37<00:00,  1.20it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create PyTorch Dataset that reads .npy features"
      ],
      "metadata": {
        "id": "L-J3KcypCvHG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "FEATURE_DIR = '/content/drive/MyDrive/ucf_small/features'\n",
        "meta = pd.read_csv('/content/drive/MyDrive/ucf_small/metadata.csv')\n",
        "\n",
        "# filter meta to keep only videos with a saved feature file\n",
        "meta['feat_exists'] = meta['video_path'].apply(lambda p: os.path.exists(os.path.join(FEATURE_DIR, os.path.splitext(os.path.basename(p))[0] + '.npy')))\n",
        "meta = meta[meta['feat_exists']].reset_index(drop=True)\n",
        "print(\"Meta rows with features:\", len(meta))\n",
        "\n",
        "# label mapping\n",
        "labels = sorted(meta['label'].unique())\n",
        "label2idx = {l:i for i,l in enumerate(labels)}\n",
        "idx2label = {i:l for l,i in label2idx.items()}\n",
        "print(\"Classes:\", len(labels), labels)\n",
        "\n",
        "# === START OF CHANGES ===\n",
        "\n",
        "# Split into Train (80%) and a temporary set (20% for Val+Test)\n",
        "train_df, temp_df = train_test_split(meta, test_size=0.2, stratify=meta['label'], random_state=42)\n",
        "\n",
        "# Split the temporary set (20%) into Validation (10%) and Test (10%)\n",
        "# We use test_size=0.5 because 0.5 * 20% = 10%\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=42)\n",
        "\n",
        "# Reset indices for all dataframes\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "print(f\"Train/Val/Test sizes: {len(train_df)} / {len(val_df)} / {len(test_df)}\")\n",
        "\n",
        "# save splits\n",
        "train_df.to_csv('/content/drive/MyDrive/ucf_small/train.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/ucf_small/val.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/ucf_small/test.csv', index=False)\n",
        "\n",
        "class FeatureDataset(Dataset):\n",
        "    def __init__(self, df, feature_dir, label2idx):\n",
        "        self.df = df\n",
        "        self.feature_dir = feature_dir\n",
        "        self.label2idx = label2idx\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        fpath = os.path.join(self.feature_dir, os.path.splitext(os.path.basename(row['video_path']))[0] + '.npy')\n",
        "        feats = np.load(fpath)  # shape (T, feat_dim)\n",
        "        return torch.tensor(feats, dtype=torch.float32), int(self.label2idx[row['label']])\n",
        "\n",
        "# dataloaders\n",
        "BATCH_SIZE = 16\n",
        "train_ds = FeatureDataset(train_df, FEATURE_DIR, label2idx)\n",
        "val_ds = FeatureDataset(val_df, FEATURE_DIR, label2idx)\n",
        "test_ds = FeatureDataset(test_df, FEATURE_DIR, label2idx) # New test dataset\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2) # New test dataloader\n",
        "\n",
        "# === END OF CHANGES ==="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1WKbLQLCtGy",
        "outputId": "204d4d49-1677-4c92-c379-48d2454507e1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta rows with features: 3360\n",
            "Classes: 25 ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress', 'Biking', 'Billiards', 'BlowDryHair', 'BlowingCandles', 'BodyWeightSquats', 'Bowling', 'BoxingPunchingBag', 'BoxingSpeedBag', 'BreastStroke', 'BrushingTeeth', 'CleanAndJerk', 'CliffDiving', 'CricketBowling', 'CricketShot', 'CuttingInKitchen']\n",
            "Train/Val/Test sizes: 2688 / 336 / 336\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM model and training loop"
      ],
      "metadata": {
        "id": "hUzQkN9dDSqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "    # Corrected the default values in the class definition\n",
        "    def __init__(self, feat_dim=2048, hidden_dim=512, num_layers=2, num_classes=25, bidirectional=False, dropout=0.7):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(feat_dim, hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers>1 else 0.0)\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * (2 if bidirectional else 1), 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        # x: (B, T, feat_dim)\n",
        "        out, (h_n, c_n) = self.lstm(x)\n",
        "        if self.bidirectional:\n",
        "            h = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
        "        else:\n",
        "            h = h_n[-1]\n",
        "        logits = self.classifier(h)\n",
        "        return logits\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "num_classes = len(labels)\n",
        "\n",
        "# === THIS IS THE CORRECTED LINE ===\n",
        "model = LSTMClassifier(num_classes=num_classes, num_layers=2, dropout=0.7).to(device)\n",
        "# ==================================\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
        "\n",
        "# training loop with early stopping\n",
        "EPOCHS = 20\n",
        "best_val_acc = 0.0\n",
        "patience = 5\n",
        "no_improve = 0\n",
        "save_path = '/content/drive/MyDrive/ucf_small/best_lstm.pth'\n",
        "\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for feats, labels_batch in tqdm(train_loader, desc=f\"Train E{epoch}\"):\n",
        "        feats = feats.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(feats)\n",
        "        loss = criterion(logits, labels_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * feats.size(0)\n",
        "        preds = logits.argmax(dim=1)\n",
        "        correct += (preds == labels_batch).sum().item()\n",
        "        total += feats.size(0)\n",
        "    train_loss /= total\n",
        "    train_acc = correct / total\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for feats, labels_batch in val_loader:\n",
        "            feats = feats.to(device)\n",
        "            labels_batch = labels_batch.to(device)\n",
        "            logits = model(feats)\n",
        "            loss = criterion(logits, labels_batch)\n",
        "            val_loss += loss.item() * feats.size(0)\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct += (preds == labels_batch).sum().item()\n",
        "            total += feats.size(0)\n",
        "    val_loss /= total\n",
        "    val_acc = correct / total\n",
        "    print(f\"Epoch {epoch}: train_loss={train_loss:.4f} train_acc={train_acc:.4f} val_loss={val_loss:.4f} val_acc={val_acc:.4f}\")\n",
        "    scheduler.step(val_acc)\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save({'model_state_dict': model.state_dict(), 'label2idx': label2idx}, save_path)\n",
        "        print(\"Saved best model.\")\n",
        "        no_improve = 0\n",
        "    else:\n",
        "        no_improve += 1\n",
        "        if no_improve >= patience:\n",
        "            print(\"Early stopping.\")\n",
        "            break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fByJg6ItEUeS",
        "outputId": "e0c88a8f-0617-40a6-bb6c-701d4ba4796f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E1: 100%|██████████| 168/168 [00:08<00:00, 19.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: train_loss=2.7753 train_acc=0.2522 val_loss=1.5802 val_acc=0.5804\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E2: 100%|██████████| 168/168 [00:08<00:00, 20.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2: train_loss=1.2852 train_acc=0.6287 val_loss=0.6145 val_acc=0.8542\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E3: 100%|██████████| 168/168 [00:09<00:00, 17.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3: train_loss=0.6479 train_acc=0.8237 val_loss=0.3269 val_acc=0.9226\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E4: 100%|██████████| 168/168 [00:09<00:00, 18.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4: train_loss=0.3993 train_acc=0.8943 val_loss=0.1888 val_acc=0.9405\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E5: 100%|██████████| 168/168 [00:10<00:00, 16.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5: train_loss=0.2600 train_acc=0.9360 val_loss=0.2326 val_acc=0.9167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E6: 100%|██████████| 168/168 [00:08<00:00, 19.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6: train_loss=0.1904 train_acc=0.9572 val_loss=0.1848 val_acc=0.9405\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E7: 100%|██████████| 168/168 [00:08<00:00, 19.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7: train_loss=0.1642 train_acc=0.9628 val_loss=0.1272 val_acc=0.9583\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E8: 100%|██████████| 168/168 [00:09<00:00, 17.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8: train_loss=0.1198 train_acc=0.9743 val_loss=0.1409 val_acc=0.9524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E9: 100%|██████████| 168/168 [00:09<00:00, 18.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9: train_loss=0.1074 train_acc=0.9777 val_loss=0.0813 val_acc=0.9673\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E10: 100%|██████████| 168/168 [00:08<00:00, 20.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10: train_loss=0.0780 train_acc=0.9847 val_loss=0.1570 val_acc=0.9583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E11: 100%|██████████| 168/168 [00:09<00:00, 18.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11: train_loss=0.0844 train_acc=0.9803 val_loss=0.0538 val_acc=0.9851\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E12: 100%|██████████| 168/168 [00:08<00:00, 18.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12: train_loss=0.1194 train_acc=0.9699 val_loss=0.0936 val_acc=0.9613\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E13: 100%|██████████| 168/168 [00:09<00:00, 17.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13: train_loss=0.0549 train_acc=0.9881 val_loss=0.0451 val_acc=0.9881\n",
            "Saved best model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E14: 100%|██████████| 168/168 [00:09<00:00, 16.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14: train_loss=0.0674 train_acc=0.9825 val_loss=0.1117 val_acc=0.9702\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E15: 100%|██████████| 168/168 [00:08<00:00, 20.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15: train_loss=0.0982 train_acc=0.9754 val_loss=0.0436 val_acc=0.9821\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E16: 100%|██████████| 168/168 [00:08<00:00, 18.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16: train_loss=0.0557 train_acc=0.9866 val_loss=0.0677 val_acc=0.9851\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E17: 100%|██████████| 168/168 [00:09<00:00, 18.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17: train_loss=0.0429 train_acc=0.9900 val_loss=0.0669 val_acc=0.9643\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train E18: 100%|██████████| 168/168 [00:08<00:00, 19.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18: train_loss=0.0231 train_acc=0.9974 val_loss=0.0274 val_acc=0.9881\n",
            "Early stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Accurcy**"
      ],
      "metadata": {
        "id": "hsk1NIQmn8BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model = LSTMClassifier(\n",
        "    num_classes=len(labels),\n",
        "    num_layers=2,\n",
        "    dropout=0.7\n",
        ").to(device)\n",
        "\n",
        "save_path = '/content/drive/MyDrive/ucf_small/best_lstm.pth'\n",
        "checkpoint = torch.load(save_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# 2. Evaluate on the test set\n",
        "model.eval()\n",
        "test_correct = 0\n",
        "test_total = 0\n",
        "with torch.no_grad():\n",
        "    for feats, labels_batch in tqdm(test_loader, desc=\"Testing\"):\n",
        "        feats = feats.to(device)\n",
        "        labels_batch = labels_batch.to(device)\n",
        "\n",
        "        logits = model(feats)\n",
        "        preds = logits.argmax(dim=1)\n",
        "\n",
        "        test_correct += (preds == labels_batch).sum().item()\n",
        "        test_total += feats.size(0)\n",
        "\n",
        "test_acc = test_correct / test_total\n",
        "print(f\"Final Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UJeo9tccNDyp",
        "outputId": "705c8fdf-1ee6-44f4-a205-cdc9977fb1f0"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 21/21 [00:01<00:00, 19.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Test Accuracy: 0.9762\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Testing"
      ],
      "metadata": {
        "id": "6giz5j2gn1rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from decord import VideoReader, cpu\n",
        "import os\n",
        "\n",
        "# =================================================================\n",
        "#  1. Setup\n",
        "# =================================================================\n",
        "\n",
        "# We assume your new video has been uploaded to the session\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "save_path = '/content/drive/MyDrive/ucf_small/best_lstm.pth'\n",
        "video_path = '/content/12433257-uhd_2160_3840_30fps.mp4' # <<< THIS LINE IS UPDATED\n",
        "NUM_FRAMES = 16\n",
        "\n",
        "# Check if the video file exists before proceeding\n",
        "if not os.path.exists(video_path):\n",
        "    print(f\"[ERROR] Video file not found at: {video_path}\")\n",
        "else:\n",
        "    # =================================================================\n",
        "    #  2. Model Loading\n",
        "    # =================================================================\n",
        "\n",
        "    resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "    resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
        "    resnet.eval().to(device)\n",
        "\n",
        "    checkpoint = torch.load(save_path)\n",
        "    label2idx = checkpoint['label2idx']\n",
        "    idx2label = {i: l for l, i in label2idx.items()}\n",
        "\n",
        "    model = LSTMClassifier(num_classes=len(label2idx)).to(device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    print(\"Models loaded successfully!\")\n",
        "\n",
        "    # =================================================================\n",
        "    #  3. Helper Functions\n",
        "    # =================================================================\n",
        "\n",
        "    def extract_frames(video_path, num_frames):\n",
        "        try:\n",
        "            vr = VideoReader(video_path, ctx=cpu(0))\n",
        "            total_frames = len(vr)\n",
        "            frame_indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
        "            frames = vr.get_batch(frame_indices).asnumpy()\n",
        "            return [Image.fromarray(frame) for frame in frames]\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading video {video_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_features(frames, feature_extractor):\n",
        "        preprocess = transforms.Compose([\n",
        "            transforms.Resize(256),\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "        features = []\n",
        "        if not frames:\n",
        "            return np.array([])\n",
        "        with torch.no_grad():\n",
        "            for frame in frames:\n",
        "                img_tensor = preprocess(frame).unsqueeze(0).to(device)\n",
        "                feat = feature_extractor(img_tensor)\n",
        "                feat = feat.squeeze(-1).squeeze(-1).cpu().numpy()\n",
        "                features.append(feat)\n",
        "        return np.vstack(features)\n",
        "\n",
        "    # =================================================================\n",
        "    #  4. Inference Execution\n",
        "    # =================================================================\n",
        "\n",
        "    print(f\"\\n[INFO] Processing video: {video_path}\")\n",
        "    frames = extract_frames(video_path, NUM_FRAMES)\n",
        "    print(f\"[INFO] Extracted {len(frames)} frames.\")\n",
        "\n",
        "    if frames:\n",
        "        features = extract_features(frames, resnet)\n",
        "        features_tensor = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "        print(f\"[INFO] Extracted features with shape: {features_tensor.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(features_tensor)\n",
        "            probabilities = torch.softmax(logits, dim=1)[0]\n",
        "            pred_idx = torch.argmax(probabilities).item()\n",
        "            pred_label = idx2label[pred_idx]\n",
        "            pred_confidence = probabilities[pred_idx].item()\n",
        "\n",
        "        print(\"\\n\" + \"=\"*30)\n",
        "        print(f\"Prediction: {pred_label.upper()}\")\n",
        "        print(f\"Confidence: {pred_confidence * 100:.2f}%\")\n",
        "        print(\"=\"*30)\n",
        "    else:\n",
        "        print(\"\\n[ERROR] Failed to extract frames. Cannot continue.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4rla1NPN0VW",
        "outputId": "84285365-8f97-4779-8067-166310cdc611"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Models loaded successfully!\n",
            "\n",
            "[INFO] Processing video: /content/12433257-uhd_2160_3840_30fps.mp4\n",
            "[INFO] Extracted 16 frames.\n",
            "[INFO] Extracted features with shape: torch.Size([1, 16, 2048])\n",
            "\n",
            "==============================\n",
            "Prediction: BLOWINGCANDLES\n",
            "Confidence: 72.66%\n",
            "==============================\n"
          ]
        }
      ]
    }
  ]
}